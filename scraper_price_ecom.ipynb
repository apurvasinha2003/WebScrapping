{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {
    "collapsed": true
   },
   "level": 1,
   "source": [
    "Webscrapping application for comparing category of electronics between walmart and amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import time \n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from operator import add\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import HTTPError\n",
    "import requests\n",
    "import sys\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Url_Item method is used to opening sub page present in each main page. The following operations are perfomed in the method:\n",
    "\n",
    "1)Open each sub page using BeautifulSoup\n",
    "\n",
    "2)Fetch walmart model No and price\n",
    "\n",
    "3)If any information scrapping fails for walmart, retry scraping.\n",
    "\n",
    "4)If walmart model no is found, proceed for Amazon search with same model no\n",
    "\n",
    "5)Scrap through amazon page and Check if the item exists in amazon, if yes scrap the price of the item. If no, attach message for the final list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_item(each,item_details,attempt):\n",
    "    #opening of sub page \n",
    "    price_compare=\"\"\n",
    "    amz_search=\"https://www.amazon.com/s/ref=nb_sb_noss?url=search-alias%3Daps&field-keywords=\"\n",
    "    try:\n",
    "        subpage = urlopen(each)\n",
    "        subpage_content = BeautifulSoup(subpage,\"lxml\")\n",
    "        subpage_modelno=subpage_content.body.find(\"div\",{\"itemprop\":\"Model\"})\n",
    "        #capturing model no for walmart\n",
    "        wal_model_no=subpage_modelno.text\n",
    "        subpage_price=subpage_content.body.find(\"span\",{\"class\":\"price-group\"})\n",
    "        #capturing price of model in walmart\n",
    "        wal_price= subpage_price.get(\"aria-label\")\n",
    "    except AttributeError as e:\n",
    "        while attempt > 0:\n",
    "            print(\"\\nError(Attempt no: \"+str(attempt)+\" for url \\'\"+ str(each)+\"\\' is : Element Tag not found\")\n",
    "            attempt -=1\n",
    "            #retry scrapping the sub page\n",
    "            item_details = url_item(each,item_details,attempt)\n",
    "    except:\n",
    "        while attempt > 0:\n",
    "            print(\"\\nError(Attempt no: \"+str(attempt)+\" for url \\'\"+ str(each)+\"\\' is : \"+str(sys.exc_info()[0]))\n",
    "            attempt -=1\n",
    "            #retry scrapping the sub page\n",
    "            item_details = url_item(each,item_details,attempt)\n",
    "    if wal_model_no is not None:# is walmart model is found, goahead and scrap amazon else skip it\n",
    "        try:\n",
    "             #goto amazon and scrap the page based on model no(fetched from walmart)\n",
    "            amz_search = amz_search+wal_model_no\n",
    "            headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36'}\n",
    "            amz_page = requests.get(amz_search,headers=headers)\n",
    "            amz_cont = BeautifulSoup(amz_page.content,\"lxml\")\n",
    "            amz_item = amz_cont.body.find(\"a\",{\"class\":\"a-link-normal a-text-normal\"})\n",
    "            amz_item_url = amz_item.get(\"href\")\n",
    "            #check if itme exist in amazon or not\n",
    "            if \"https://www.amazon.com\" not in amz_item_url:\n",
    "                price_compare = wal_price+\"~Product Not Found\"\n",
    "                #print(\"Not found url\")\n",
    "            else:\n",
    "                item_page = requests.get(amz_item_url,headers=headers)\n",
    "                item_cont = BeautifulSoup(item_page.content,\"lxml\")\n",
    "                amz_price = (item_cont.body.find(\"span\",{\"class\":\"a-size-medium a-color-price\"})) #or (item_cont.body.find(\"div\",\"class\" : \"a-section a-spacing-small a-spacing-top-small\")) \n",
    "                price_compare = str(wal_price) + \"~\" + str(amz_price.text.strip())\n",
    "            #append the model no, price into the final list    \n",
    "            item_details.append(str(wal_model_no)+\"~\"+ str(price_compare))  \n",
    "        except HTTPError as e:\n",
    "            print(\"\\nAmazon product not found for url \\'\"+ str(amz_search)+\"\\' is : \"+str(e))\n",
    "        except:\n",
    "            print(\"\\nAmazon product not found for url \\'\"+ str(amz_search)+\"\\' is : \"+str(sys.exc_info()[0]))\n",
    "      \n",
    "    return(item_details) "
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Method for processing Main page and calling url_item() method to process each sub page . It builds an output dataset having the model no and price from walmart and Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(url):\n",
    "    call_link=[] #individual page content\n",
    "    item_details=[]\n",
    "    try:\n",
    "        #opening of page url under the main page\n",
    "        page = urlopen(url)\n",
    "        url_content = BeautifulSoup(page,\"lxml\")\n",
    "    \n",
    "##page content\n",
    "        all_link = url_content.body.find_all(\"a\",class_= desc)\n",
    "        for link in all_link:\n",
    "            call_link.append(subpage_url+link.get(\"href\"))\n",
    "        #opening sub pages\n",
    "        for each in call_link:\n",
    "            attempt = 2\n",
    "            if each is None:\n",
    "                continue\n",
    "            try:\n",
    "               item_details = url_item(each,item_details,attempt) \n",
    "            except:\n",
    "                print(\"\\nError for url \\'\"+ str(each)+\"\\' is : \"+str(sys.exc_info()[0]))\n",
    "    except:\n",
    "        print(\"error\"+str(sys.exc_info()[0]))\n",
    "    return item_details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of items\n",
    "category = ['microwave','juicer','TV','Cellphones','laptops']   \n",
    "for each in category:\n",
    "    url = \"https://www.walmart.com/search/?query=\"+each+\"&cat_id=0\"\n",
    "    desc=[]\n",
    "    desc.append('product-title-link')\n",
    "    desc.append('line-clamp')\n",
    "    \n",
    "    desc.append('line-clamp-2')\n",
    "    subpage_url=\"https://www.walmart.com\"\n",
    "    wal_url = \"https://www.walmart.com/search/?query=\"+each+\"&page=\"\n",
    "    wal_end_url = \"&cat_id=0&grid=true&ps=40\"\n",
    "    call_link=[] #individual page content\n",
    "    new_page=[]#page url\n",
    "    list_item = {}\n",
    "    page = urlopen(url)\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "    #retrieving page nos\n",
    "    paginator = soup.body.find(class_=\"paginator-list\")\n",
    "    page_no = paginator.find(class_= \"active\")\n",
    "    page_no = page_no.get(\"aria-label\").split()[3]\n",
    "    print(\"total no of pages \"+ str(page_no))\n",
    "    #forming page url\n",
    "    for i in range(1,int(page_no)+1):\n",
    "        new_page.append(wal_url+str(i)+wal_end_url)\n",
    "    print(\"length is\" + str(len(new_page)))\n",
    "    #creation of rdd\n",
    "    start = time.time()\n",
    "    spark = SparkContext.getOrCreate()\n",
    "    spark.setLogLevel(\"WARN\")\n",
    "    rdd1 = spark.parallelize(new_page)\n",
    "    rdd2 = rdd1.repartition(4)\n",
    "    list_item = rdd2.flatMap(lambda url : process(url))\n",
    "    #saving outfile file for each catgeory with current time stamp\n",
    "    list_item.saveAsTextFile('/price_webscrapping_'+str(datetime.datetime.now())[0:16].replace(':','_').replace(' ','_')+'/')\n",
    "    #prints out the time taken to process each category\n",
    "    print(\"Total time is \" + str(time.time() - start) + \" in ms\" )\n",
    "    \n",
    "    #end of rdd creation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
